# ğŸš€ Ollama + MindSphere Kurulum Rehberi (macOS)

Bu rehber, macOS Ã¼zerinde Ollama kurulumu ve MindSphere projesi ile entegrasyonu iÃ§in adÄ±m adÄ±m talimatlar iÃ§erir. Bu sayede tamamen **Ã¼cretsiz** ve **yerel** AI sistemi kurabilirsiniz!

## ğŸ“‹ Ä°Ã§indekiler

1. [Sistem Gereksinimleri](#sistem-gereksinimleri)
2. [Ollama Kurulumu](#ollama-kurulumu)
3. [Model Ä°ndirme](#model-iÌ‡ndirme)
4. [MindSphere Entegrasyonu](#mindsphere-entegrasyonu)
5. [Model YÃ¶netimi](#model-yÃ¶netimi)
6. [Performans Optimizasyonu](#performans-optimizasyonu)
7. [Sorun Giderme](#sorun-giderme)

---

## ğŸ–¥ï¸ Sistem Gereksinimleri

### Minimum Gereksinimler:
- **macOS 10.15** (Catalina) veya Ã¼zeri
- **8 GB RAM** (16 GB Ã¶nerilir)
- **10 GB boÅŸ disk alanÄ±** (kÃ¼Ã§Ã¼k modeller iÃ§in)
- **Intel/Apple Silicon** iÅŸlemci

### Ã–nerilen Gereksinimler:
- **macOS 12** (Monterey) veya Ã¼zeri
- **16 GB+ RAM** (bÃ¼yÃ¼k modeller iÃ§in)
- **50 GB+ boÅŸ disk alanÄ±** (birden fazla model iÃ§in)
- **Apple Silicon (M1/M2/M3)** iÅŸlemci

---

## ğŸ”§ Ollama Kurulumu

### YÃ¶ntem 1: Homebrew ile Kurulum (Ã–nerilen)

```bash
# Homebrew kurulu deÄŸilse Ã¶nce kurun:
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Ollama'yÄ± kurun:
brew install ollama

# Ollama servisini baÅŸlatÄ±n:
brew services start ollama
```

### YÃ¶ntem 2: Resmi Installer ile Kurulum

1. [Ollama resmi sitesini](https://ollama.com) ziyaret edin
2. **Download for macOS** butonuna tÄ±klayÄ±n
3. Ä°ndirilen `.dmg` dosyasÄ±nÄ± aÃ§Ä±n
4. Ollama'yÄ± Applications klasÃ¶rÃ¼ne sÃ¼rÃ¼kleyin
5. Spotlight'tan "Ollama" arayarak Ã§alÄ±ÅŸtÄ±rÄ±n

### Kurulum DoÄŸrulama

Terminal aÃ§Ä±p ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
ollama --version
```

SÃ¼rÃ¼m bilgisi gÃ¶rÃ¼nÃ¼yorsa kurulum baÅŸarÄ±lÄ±dÄ±r!

---

## ğŸ“¦ Model Ä°ndirme

### Ã–nerilen Modeller

#### ğŸš€ BaÅŸlangÄ±Ã§ Ä°Ã§in (8-16 GB RAM)
```bash
# Llama 3.1 8B - En popÃ¼ler ve dengeli model
ollama pull llama3.1:8b

# Mistral 7B - HÄ±zlÄ± ve etkili
ollama pull mistral:7b
```

#### ğŸ’ª GÃ¼Ã§lÃ¼ Performans Ä°Ã§in (16+ GB RAM)
```bash
# Llama 3.1 70B - En yÃ¼ksek kalite
ollama pull llama3.1:70b

# Code Llama - Kod yazma iÃ§in Ã¶zel
ollama pull codellama:13b
```

#### ğŸ–¼ï¸ GÃ¶rsel Analiz Ä°Ã§in
```bash
# LLaVA - GÃ¶rÃ¼ntÃ¼ analizi yapabilen model
ollama pull llava:13b
```

### Model BoyutlarÄ± ve Gereksinimler

| Model | Boyut | RAM Gereksinimi | KullanÄ±m AlanÄ± |
|-------|-------|----------------|-----------------|
| `llama3.1:8b` | ~4.7 GB | 8 GB | Genel amaÃ§lÄ±, hÄ±zlÄ± |
| `mistral:7b` | ~4.1 GB | 8 GB | HÄ±zlÄ± yanÄ±tlar |
| `llama3.1:70b` | ~40 GB | 64 GB | En yÃ¼ksek kalite |
| `codellama:13b` | ~7.3 GB | 16 GB | Kod yazma |
| `llava:13b` | ~7.3 GB | 16 GB | GÃ¶rÃ¼ntÃ¼ analizi |

---

## ğŸ”— MindSphere Entegrasyonu

### 1. Ollama Servisini BaÅŸlatma

```bash
# Ollama'yÄ± arka planda Ã§alÄ±ÅŸtÄ±rÄ±n:
ollama serve
```

**Not:** Bu komut terminali aÃ§Ä±k tutacaktÄ±r. Arka planda Ã§alÄ±ÅŸtÄ±rmak iÃ§in:

```bash
# macOS iÃ§in LaunchAgent olarak Ã§alÄ±ÅŸtÄ±rma:
brew services start ollama
```

### 2. MindSphere KonfigÃ¼rasyonu

MindSphere projesindeki `.env` dosyasÄ±nda:

```env
OLLAMA_URL=http://localhost:11434
```

**Bu ayar zaten projenizde mevcut!** Ekstra konfigÃ¼rasyon gerekmez.

### 3. BaÄŸlantÄ± Testi

1. MindSphere uygulamasÄ±nÄ± baÅŸlatÄ±n:
   ```bash
   npm run dev
   ```

2. TarayÄ±cÄ±da `http://localhost:8000/preferences` adresine gidin

3. **AI Providers** sekmesine tÄ±klayÄ±n

4. **Local LLM (Ollama)** kartÄ±nda ÅŸu bilgileri gÃ¶rmelisiniz:
   - âœ… **Online** durumu
   - ğŸ”¢ Ä°ndirdiÄŸiniz model sayÄ±sÄ±
   - ğŸš€ **No API key required!** yazÄ±sÄ±

### 4. Test Etme

1. AI Providers sayfasÄ±nda Local LLM kartÄ±nda **"Test Connection"** butonuna tÄ±klayÄ±n

2. BaÅŸarÄ±lÄ± olursa:
   ```
   âœ… Connection successful!
   local_llm API key is working. Response time: XXXms
   ```

---

## ğŸ›ï¸ Model YÃ¶netimi

### YÃ¼klÃ¼ Modelleri Listeleme
```bash
ollama list
```

### Model Ä°ndirme
```bash
# Yeni model indirme:
ollama pull [model-adÄ±]

# Ã–rnek:
ollama pull phi3:mini
```

### Model Silme
```bash
# Gereksiz modelleri silme:
ollama rm [model-adÄ±]

# Ã–rnek:
ollama rm llama2:7b
```

### Model DetaylarÄ±
```bash
# Model bilgilerini gÃ¶rme:
ollama show [model-adÄ±]

# Ã–rnek:
ollama show llama3.1:8b
```

### Disk AlanÄ± Temizleme

```bash
# KullanÄ±lmayan modelleri temizleme:
ollama prune

# TÃ¼m cache'i temizleme:
rm -rf ~/.ollama/models/manifests/*
```

---

## âš¡ Performans Optimizasyonu

### 1. RAM Optimizasyonu

**KÃ¼Ã§Ã¼k modeller iÃ§in (8 GB RAM):**
```bash
# Sadece kÃ¼Ã§Ã¼k modelleri kullanÄ±n:
ollama pull llama3.1:8b
ollama pull mistral:7b
```

**BÃ¼yÃ¼k modeller iÃ§in (16+ GB RAM):**
```bash
# BÃ¼yÃ¼k modelleri Ã§alÄ±ÅŸtÄ±rÄ±rken diÄŸer uygulamalarÄ± kapatÄ±n
ollama pull llama3.1:70b
```

### 2. CPU/GPU Optimizasyonu

Apple Silicon iÃ§in otomatik GPU ivmelendirilmesi aktiftir. Manuel ayar gerekmez.

### 3. Model DeÄŸiÅŸtirme

MindSphere'de varsayÄ±lan olarak en uygun model otomatik seÃ§ilir:
1. `llama3.1:latest`
2. `llama3.1:8b`
3. `llama2:latest`
4. `mistral:latest`

### 4. BaÄŸlantÄ± Optimizasyonu

Ollama'nÄ±n her zaman Ã§alÄ±ÅŸÄ±r durumda olmasÄ± iÃ§in:

```bash
# System startup'ta otomatik baÅŸlatma:
brew services start ollama

# Manuel baÅŸlatma:
ollama serve &
```

---

## ğŸ”§ Sorun Giderme

### Problem 1: "Connection failed" HatasÄ±

**Ã‡Ã¶zÃ¼m:**
```bash
# Ollama Ã§alÄ±ÅŸÄ±yor mu kontrol edin:
ps aux | grep ollama

# Ã‡alÄ±ÅŸmÄ±yorsa baÅŸlatÄ±n:
ollama serve

# Veya servis olarak:
brew services start ollama
```

### Problem 2: "No models available" HatasÄ±

**Ã‡Ã¶zÃ¼m:**
```bash
# Model listesini kontrol edin:
ollama list

# Model yoksa indirin:
ollama pull llama3.1:8b
```

### Problem 3: Port Ã‡akÄ±ÅŸmasÄ± (11434)

**Port kullanÄ±mÄ± kontrol:**
```bash
lsof -i :11434
```

**FarklÄ± port kullanma:**
```bash
# Ollama'yÄ± farklÄ± portta Ã§alÄ±ÅŸtÄ±rma:
OLLAMA_HOST=0.0.0.0:11435 ollama serve
```

**.env dosyasÄ±nda da gÃ¼ncelleyin:**
```env
OLLAMA_URL=http://localhost:11435
```

### Problem 4: YavaÅŸ YanÄ±tlar

**Ã‡Ã¶zÃ¼mler:**
1. **Daha kÃ¼Ã§Ã¼k model kullanÄ±n:**
   ```bash
   ollama pull mistral:7b-instruct
   ```

2. **RAM'i kontrol edin:**
   ```bash
   # Bellek kullanÄ±mÄ±nÄ± kontrol:
   memory_pressure
   ```

3. **DiÄŸer uygulamalarÄ± kapatÄ±n**

### Problem 5: Model Ä°ndirme HatasÄ±

**Network sorunlarÄ± iÃ§in:**
```bash
# Proxy ayarlarÄ±:
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port

# DNS ayarlarÄ±:
sudo dscacheutil -flushcache
```

### Problem 6: MindSphere Provider Offline

**Kontrol adÄ±mlarÄ±:**
1. **Ollama durumunu kontrol:**
   ```bash
   curl http://localhost:11434/api/version
   ```

2. **Model listesini kontrol:**
   ```bash
   curl http://localhost:11434/api/tags
   ```

3. **MindSphere servisini yeniden baÅŸlat:**
   ```bash
   # MindSphere dev server'Ä± yeniden baÅŸlatÄ±n
   npm run dev
   ```

---

## ğŸ¯ KullanÄ±m SenaryolarÄ±

### 1. MindSphere Chat

1. **Dashboard**'a gidin
2. **Chat** sekmesine tÄ±klayÄ±n
3. Local LLM otomatik olarak kullanÄ±lacak
4. Tamamen Ã¼cretsiz ve offline Ã§alÄ±ÅŸÄ±r!

### 2. Journal Analizi

1. **Journal** sayfasÄ±na gidin
2. GÃ¼nlÃ¼k kaydÄ±nÄ±zÄ± yazÄ±n
3. Local LLM otomatik analiz yapacak
4. KiÅŸisel verileriniz yerel kalÄ±r

### 3. SaÄŸlÄ±k Verisi Ä°nceleme

1. **Health** verilerinizi girin
2. Local LLM analiz ve Ã¶neriler sunar
3. Verileriniz internete Ã§Ä±kmaz

---

## ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Ã–zellik | Llama 3.1 8B | Mistral 7B | Llama 3.1 70B |
|---------|---------------|------------|----------------|
| **HÄ±z** | â­â­â­â­ | â­â­â­â­â­ | â­â­ |
| **Kalite** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| **RAM Gereksimi** | 8 GB | 8 GB | 64 GB |
| **Disk Boyutu** | 4.7 GB | 4.1 GB | 40 GB |
| **TÃ¼rkÃ§e DesteÄŸi** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |

### Ã–neri:
- **BaÅŸlangÄ±Ã§:** `llama3.1:8b`
- **HÄ±z odaklÄ±:** `mistral:7b`
- **En iyi kalite:** `llama3.1:70b` (gÃ¼Ã§lÃ¼ sistem gerekli)

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§ (5 Dakika)

```bash
# 1. Ollama'yÄ± kurun
brew install ollama

# 2. Ollama'yÄ± baÅŸlatÄ±n
brew services start ollama

# 3. Model indirin
ollama pull llama3.1:8b

# 4. MindSphere'i baÅŸlatÄ±n
npm run dev

# 5. Test edin
# http://localhost:8000/preferences â†’ AI Providers â†’ Test Connection
```

**Tebrikler! ğŸ‰ ArtÄ±k tamamen Ã¼cretsiz, yerel AI sisteminiz hazÄ±r!**

---

## ğŸ“š Ek Kaynaklar

- [Ollama Resmi DokÃ¼mantasyon](https://ollama.com/docs)
- [Model Hub](https://ollama.com/library)
- [MindSphere GitHub Repo](https://github.com/anthropics/mindsphere)

---

## ğŸ’¡ Ä°puÃ§larÄ±

1. **Multiple Models:** FarklÄ± gÃ¶revler iÃ§in farklÄ± modeller kullanabilirsiniz
2. **Updates:** `ollama pull model-name` ile modelleri gÃ¼ncelleyebilirsiniz
3. **Monitoring:** `ollama ps` ile Ã§alÄ±ÅŸan modelleri gÃ¶rebilirsiniz
4. **Backup:** `~/.ollama/models/` klasÃ¶rÃ¼nÃ¼ yedekleyebilirsiniz

---

**ğŸ”’ GÃ¼venlik:** TÃ¼m verileriniz yerel kalÄ±r, internete Ã§Ä±kmaz!
**ğŸ’° Maliyet:** Tamamen Ã¼cretsiz, API limiti yok!
**âš¡ Performans:** Apple Silicon'da optimize edilmiÅŸ Ã§alÄ±ÅŸÄ±r!